---
title: "Bayesian methods in Extreme Value Theory"
author: "Antoine Pissoort"
date: "March 13, 2017"
categories: ["R"]
tags: ["Bayesian", "Gibbs Sampler", "ggplot2"]
includes:
  before_body: [preamble-mathjax.tex, bracket-start.txt, preamble-both.tex, bracket-end.txt]
output:
  blogdown::html_page()
bibliography: sum.bib
---

```{r setup, message=F, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T, fig.pos = "c")
load("/home/proto4426/Documents/master_Thesis/Extreme/R resources/IRM/data1.Rdata")
library(evd)
library(mvtnorm)
library(KernSmooth)
library(coda)
library(pander)
library(bayesplot)
library(tidyverse)
```



We found that *evdbayes* package typically uses the *Metropolis-Hastings* (MH) algorithm as MCMC sampler. We are aware that this probably not the most efficient algorithm available in the literature, but it is "easy" to implement and understand.
**Beware** : We do not know if it is either the MH or the Gibbs sampler which is implemented when doing simulations with this package. [@hartmann_bayesian_2016] state in their article that it is the MH but in the package's source functions we see that this is rather the Gibbs sampler. We found no information about it somewhere else provided in the package....

However, we will try to (compare and to) rely on other ways than this sole package, e.g. 

**1.** Implement our *own functions*. The idea is to better understand the "black-box" and the hidden Bayesian's mechanism, which is difficult when using only package's functions. Moreoever, it will allow us to implement other algorithm (MH or Gibbs), to have better flexibility,  ...
We will be mainly based on the book [@dey_extreme_2016,chapter 13]. 

**2.** *Hamiltonian Monte Carlo* based mainly on the same article [@hartmann_bayesian_2016] (...). The objective is then to use the *Stan* language which makes use of this technique, and which is built with the compiled language c++. This is (really) more efficient and thus would be preferable.

**3.** *revdbayes* ? Using sample ratio of uniforms (...) Not yet studied




**Functions that we will use for the Bayesian setting** :

Look at the `PissoortThesis` package

```{r}
library(PissoortThesis)
```


Notice that we will use **non-informative**, i.e. large variance **priors** (so far?) in the following.

# First implementations : stationary GEV setting


## Metropolis-Hastlings   

It is recommended to target *an acceptance rate of around 0.25 when all components of* $\theta$ *are updated simultaneously, and 0.40 when the components are updated one at a time.*

```{r }

# Optimize Posterior Density Function to find starting values
fn <- function(par, data) -log_post0(par[1], par[2], par[3], data)
param <- c(mean(max_years$df$Max),log(sd(max_years$df$Max)), 0.1 )
# opt <- optim(param, fn, data = max_years$data, 
#              method="BFGS", hessian = TRUE)
opt <- nlm(fn, param, data = max_years$data,
           hessian=T, iterlim = 1e5) 
start1 <- opt$estimate
Sig <- solve(opt$hessian)
ev <- eigen( (2.4/sqrt(2))^2 * Sig)
varmat <- ev$vectors %*% diag(sqrt(ev$values)) %*% t(ev$vectors)

set.seed(100)
iter <- 
mh.mcmc1 <- MH_mcmc.own(start1, varmat %*% c(.36,.46,.57))
cat(paste("acceptance rate is   ", round(mh.mcmc1$mean.acc_rates,5 )))

colnames(mh.mcmc1$out.chain) <- c("mu", "logsig", "xi", "iter")
chains.plotOwn(mh.mcmc1$out.chain)
```

We can visually verify that here, the posterior's components (=parameters) are update simultaneously, i.e. jumps occur at the same time. (there are of course still not of the same "size", this not veery visible here) 

However, it seems OK. Chains are good, it mix well, and acceptance rate is around 0.25,.... Parameter space could seem quite correctly visited.

**Beware**: add *Burn-in* should be more prudent. We will handle that in the following. 

##  GIBBS sampler 

```{r}
set.seed(100)
iter <- 2000
gibb1 <- gibbs_mcmc.own(list(start1), iter = iter) # Same starting point as MH 

cat("acceptance rates are ")
round(gibb1$mean_acc.rates[[1]], 5 )

# Do not forget Burn in period (We will make it inside function in  following)
burn <- iter/4  # Tune value

gibb1$out.chain <- gibb1$out.chain[-(1:burn),]


chains.plotOwn(gibb1$out.chain)

```

Here, we can first verify by eyes that the parameters are updated independently, that is one at time. It means that we generate different proposals for each parameter's update, see above `gibbs_mcmc.own()`.

Chains seem well stationnary. Acceptance rates are good too. This is, again,  managed by a tuning of the proposal's variance , made by trial-and-error (so far).


For example, we could then see what **rough estimate** this would yield. We averaged the value  over the chains, and compare it  with those obtained by the same  frequentist method (GEV)  
```{r, results = 'asis'}
param_gibbs <- apply(gibb1$out.chain[,1:3], 2, mean) # Average over the (3) generated chains
param_gibbs["logsig"] <- exp(param_gibbs["logsig"] ) 

frame <- data.frame(Bayesian = param_gibbs, 'Frequentist(mle)' = gev_fit$mle)
row.names(frame) = c("$\\mu \\ $", "$\\sigma \\quad$", "$\\xi \\quad$")
knitr::kable(frame, align = 'l')

```

These estimates are very close (...)

However, we will now consider our **last model**, by adding the significant linear trend, and then we will do all the (convergence) diagnostics needed before making any inference.



# Gibbs Sampler with **Nonstationarity** (GEV, linear trend)

From now, this will be our final model ! We will then "expand" a bit more.


**1.  We optimize log-posterior to retrieve (good) starting values from it**
```{r}
data <- max_years$data


fn <- function(par, data) -log_post1(par[1], par[2], par[3],
                                        par[4], data)
param <- c(mean(max_years$df$Max), 0, log(sd(max_years$df$Max)), -0.1 )
opt <- optim(param, fn, data = max_years$data,
             method = "BFGS", hessian = T)
cat(paste("Optimized starting values are : \n")) 
print <- opt$par
names(print) <- c("mu", "mu1", "logsig", "xi")
print
```

However, we will run several chains with this algorithms to improve convergence properties. Hence, we will have several starting values. These starting values will thus be put inside a *random generator of starting starting values* $\Longrightarrow\downarrow$ 


**2. Choose several sets of Starting Values randomly :**

This enables to run several different chains. This will be useful for further assesment of the convergence. These sets of starting values must be (over)dispersed to ensure the visit of the whole parameter space. 
**Note** that these have to be (re)tuned.

```{r}
set.seed(100)
start <- list() ; k <- 1 # Put them on a list
while(k < 5) { # starting value is randomly selected from a distribution
  # that is overdispersed relative to the target
  sv <- as.numeric(rmvnorm(1, opt$par, 50 * solve(opt$hessian)))
  svlp <- log_post1(sv[1], sv[2], sv[3], sv[4], max_years$data)
  #print(svlp)
  if(is.finite(svlp)) {
    start[[k]] <- sv
    names(start[[k]]) <- c("mu", "mu1", "logsig", "xi")
    k <- k + 1
  }
}
knitr::kable(matrix(unlist(start), ncol = 4, byrow = T, dimnames = list(c("start[[1]]", "start[[2]]", "start[[3]]", "start[[4]]"), c("$\\mu \\ $","$\\mu_{trend}$", "$\\sigma$", "$\\xi$"))), align = "c")

```

Somewhat arbitrarily here, we go for **4 different chains** inside the function.  

**3. Run the final algorithm** 

The number of components in the `list()` "start" will automatically define the number of chains generated.

```{r}
# k chains with k different starting values
set.seed(100)
gibbs.trend <- gibbs.trend.own(start, propsd = c(.5, 1.9, .15, .12),
                               iter = 1000) # Number of iter is for each chain. 
acc_rates.param <- colMeans(do.call(rbind, gibbs.trend$mean_acc.rates))
cat("acceptance rates are :")
round(acc_rates.param, 5 )

param.chain <- gibbs.trend$out.chain[ ,1:4]
```

It runs relatively fast and the acceptance rates are all close to the target  $\approx 0.4$. 

**NOTE** : 

* **Proposal**'s standard deviation default in the function is from a trial-and-error . This has to be (re)-tuned or adapted. We could find more automatic way to achieve this task.
* **Burn-in**'s period is done into the function. This also can be tuned, whether by changing inside the function or by adding parameter in the function. Hence, we have ran 1000 iterations for each chains, and deleted half of each. We are thus again left with 2000 posterior samples. Note that it seems not necessary here to burn so much. 
* Number of **iterations** can also be increased for more precision (...). Algorithm is relatively efficient.
* 

**Plot of the so-obtained (complete) chains**
```{r, fig.height= 6.5}
colnames(gibbs.trend$out.chain) <- c("mu", "mu1", "logsig", "xi", "chain.nbr", "iter")

PissoortThesis::chains.plotOwn(gibbs.trend$out.chain )
```


```{r, fig.height=2.3}
ggplot(gibbs.trend$out.chain) + geom_line(aes(x = iter, y = mu1)) + theme_piss(16,14) + labs(ylab = "mu1", xlab = "iter" ) +  geom_hline(aes(yintercept = mean(gibbs.trend$out.chain$mu1), col = "Posterior mean"), linetype = "dashed", size = 0.7)
```

Mixing properties look relatively good for each of the parameter's chains. 
Even for $\mu_{trend}$ (last one), as compared with our fail with  `evdbayes package` ! (...) 

Now, we must go further with **diagnostics**. 



## Diagnostics 

No diagnostics can assess convergence without uncertainty. Then, we must use several relevant tools to increase our confidence that convergence indeed happened, i.e. the equilibrium distribution has been reached.

We will now rely on some packages to do this task, namely the well-known `coda` and the `bayesplot`. While the former is present since a moment, the latter is a new great visual tool relying on `ggplot`. It is mainly used for STAN outputs but it can be used here too, after some structural refinements... but we we did not achieve it so far.


Traceplots of the chains with **different starting values :**


```{r, fig.height=2, echo = T}
colnames(gibbs.trend$out.chain)[1] <- "mu0"
chain.mix <- cbind.data.frame(gibbs.trend$out.chain,
                              iter.chain = rep(1:500, 4))
g <- mixchains.Own(chain.mix)
g$gmu
g$gmutrend
g$glogsig
g$gxi
```


Chain mixing seem quite well meaning that, whatever the starting value we use, which are actually very broad (see above), the behaviour of the generated chains are similar, in the sense that they are "reaching" in the same way the targetted stationary distribution.


**Gelman-Rubin (Coda) Diagnostics**

This diagnostic compares the behaviour of the several randomly initialized chains. It uses the *potential scal reduction statistic* (or *shrink factor*) $\hat{R}$ which measures now **quantitatively** the mixing of the chains. To do that, It measures the ratio of the average variance of samples within each chain to the variance of the pooled samples accross chains. If convergence occured, these should be the same and $\hat{R}$ will be 1. If not, this will be >1. 

```{r}
# Function to create mcmc.lists, useful for diagnostics on chains.
'mc.listDiag4' <- function (list, subset = c("mu0", "mu1", "logsig", "xi")) {
  mcmc.list(mcmc(list[[1]][, subset]),
            mcmc(list[[2]][, subset]),
            mcmc(list[[3]][, subset]),
            mcmc(list[[4]][, subset])
  )
}
```


```{r}
## Gelman Coda Diagnostics : Base plot
Rhat <- gelman.diag(mc.listDiag4(gibbs.trend$out.ind), autoburnin=F)
gelman.plot(mc.listDiag4(gibbs.trend$out.ind), autoburnin=F, auto.layout = T)

##  In ggplot : Put all on the same y-scales
gp.dat <- gelman.plot(mc.listDiag4(gibbs.trend$out.ind), autoburnin=F)
df = data.frame(bind_rows(as.data.frame(gp.dat[["shrink"]][,,1]),
                          as.data.frame(gp.dat[["shrink"]][,,2])),
                q = rep(dimnames(gp.dat[["shrink"]])[[3]],
                      each = nrow(gp.dat[["shrink"]][,,1])),
                last.iter = rep(gp.dat[["last.iter"]], length(gp.dat)))
library(reshape2)
df_gg <- melt(df, c("q","last.iter"), value.name = "shrink_factor")
ggplot(df_gg,
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  geom_hline(yintercept = 1.1, colour = "green4", linetype = "dashed", size = 0.3) +
  scale_y_continuous(breaks = c(1, 1.1, 1.5, 2, 3, 4 ),
                     labels = c(1, 1.1, 1.5, 2, 3, 4 )) +
  #ggtitle("Gelman Rubin dignostic : R-hat Statistic") +
  facet_wrap(~variable,
             labeller= labeller(.cols=function(x) gsub("V", "Chain ", x))) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile",
       subtitle = "Gelman Rubin diagnostic : R-hat Statistic") +
  scale_linetype_manual(values=c(2,1)) +
  theme_piss() +
  theme(strip.text = element_text(size=15),
        plot.subtitle = element_text(size = 21, hjust = 0.5,
                                     colour = "#33666C", face = "bold"))

```

**(look at y scales !!!!!)**

We can see that it is quite close to 1, for every chains and for this ('small') number of iterations. The common rule is to be more prudent whenever $\hat{R}>1.1$.
We remark that for $\xi$, it seems to take more iterations before reaching stationary distribution.


Note that we did not make any *thinning* so far, i.e. we did not take only one simulation every *thin* number of simulations. This process is  widely used in the litterature and it is useful to reduce autocorrelations through the chains. However, it has also been proven that *thinning* the chains is actually less efficient than keeping all the generated samples for inference... The greater sample size effect of no thinning is generally stronger than the autocorrelation reduce factor of thinning
It could be easily implemented.

**Markov Chain's autocorrelations :**

This handles correlations **within a single parameter's chain**. As for cross-correlations, we are looking here for small values for good properties.

```{r}
# Transform back the scale parameter
param.chain[, "sigma"] <- exp(param.chain[,"logsig"])

## Markov Chain Correlations
#autocorr(mcmc(param.chain[, c("mu0", "mu1", "logsig", "xi")] ))
#autocorr.diag(mcmc(param.chain[, c("mu0", "mu1", "logsig", "xi")] ))
autocorr.plot(mcmc(param.chain[, c("mu0", "mu1", "logsig", "xi")]  ))
```


We can see that the autocorrelation behaviour for the chains seem quite fine.

Again, we can remark that this is a bit more "slow" for $\xi$, as we could expect. 


**Cross-correlations : **

This one handles the correlations **accross parameter**

```{r}
crosscorr.plot(mcmc(param.chain[, c("mu0", "mu1", "logsig", "xi")]  ),
                title = "Cross-correlation")
title("Cross-correlation")
# In ggplot
library(ggcorrplot)
ggcorrplot(crosscorr(mcmc(param.chain[, c("mu0", "mu1", "logsig", "xi")])),
           hc.order = TRUE, type = "lower", lab = TRUE, title = "Cross-correlation",
           ggtheme = PissoortThesis::theme_piss)
# Compare it with Fisher information matrix ('frequentist') by MLE with ismev
cr.corr_lin <- crosscorr(mcmc(param.chain[, c("mu0", "mu1", "sigma", "xi")]))
dimnames(gev_nonstatio$cov) <- dimnames(cr.corr_lin)
# Transform it to correlation for comparison
cov2cor(gev_nonstatio$cov)  ;    cr.corr_lin
```

As usual, we can remark that this is only the cross-correlations associated to $\xi$ which retain our attention.  

Could we check how this dependence can be intuitively explained in comparison with frequentist's GEV ?  


**Geweke Diagnostics :**

In short, this diagnostic, for each chains, tests for equality of the first $10\%$ of a single parameter's chain with the mean computed from the 2nd half of the chain. Large buffer between these 2 blocks are taken to assume these are probably independent. We then do a classical (frequentist..) z-score test for equality of the 2 means based on the effective sample sizes which account for autocorrelations (...) 

```{r}
geweke <- geweke.diag(mcmc(param.chain))
2*dnorm(geweke$z) 
```

Now, we partitionned the first half into 20 segments of iterations,

```{r, fig.height=6, fig.width=6}
geweke.plot(mcmc(param.chain), nbins = 20) 
```

Maybe a few **more iterations** could not be too much to convince ourself that convergence did really occur, especially again for $\xi$ for which the p-value is not far from the common level of $5\%$ .


**Raftery and Lewis's Diagnostics**

This is a run length control diagnostic based on a criterion of accuracy of estimation of a quantile q. It also informs if the number of iterations is too small.

For example, here for the quantile $5\%$ with a precision of 95% and an margin's error of 2%, i.e. estimate $q_{0.05}\pm 2\%$ with $95\%$ accuracy. 

We did it for each chains, and here are the results, for the 4 chains separately :

```{r}
## Raftery Coda Diagnostics
# For each chain individually
raf1 <- raftery.diag(mc.listDiag4(gibbs.trend$out.ind)[[1]], q=0.05, r=0.02, s=0.95)
raf2 <- raftery.diag(mc.listDiag4(gibbs.trend$out.ind)[[2]], q=0.05, r=0.02, s=0.95)
raf3 <- raftery.diag(mc.listDiag4(gibbs.trend$out.ind)[[3]], q=0.05, r=0.02, s=0.95)
raf4 <- raftery.diag(mc.listDiag4(gibbs.trend$out.ind)[[4]], q=0.05, r=0.02, s=0.95)
set.seed(12)
raf <- sample(list(raf1$resmatrix, raf2$resmatrix, raf3$resmatrix, raf4$resmatrix), 1)

pander(as.data.frame(raf))

# For the complete chains
raf_tot <- raftery.diag(mcmc(gibbs.trend$out.chain[, c("mu0", "mu1", "logsig", "xi")]),
                     q=0.05, r=0.02, s=0.95)
pander(raf_tot$resmatrix)
```

These values are based on the (auto)correlation inside the generated samples and it informs about minimum values required for a chain with no correlation between consecutive samples. 

1. **Burn-in** number of deleted values is small, meaning that starting values are not very influent.
2. **Total**  is the advised number of iterations, which is actually quite close to our "choice" (=2000) 
3. **Lower bound** is the minimum sample size based on zero autocorrelation. Here it is relatively low, so it is a good point.
4. **Dependence factor** informs about the dependence into the chains, or the extent to which autocorrelation inflates the required sample size. It is common to say that values above 5for this criterion indicate a strong autocorrelation. Here we see that it is slightly the case, especially for... $\xi$. 
<br>


## Final Results

**Summary Table from the posterior :**

```{r}
tab1 <- as.data.frame(summary(mcmc(param.chain))$quantiles)
#colnames(tab1) <- c("$\\boldsymbol{q_{0.025}}$","$\\boldsymbol{q_{0.25}}$","Median","$\\boldsymbol{q_{0.75}}$","$\\boldsymbol{q_{0.975}}$")
#rownames(tab1) <- c("$\\mu \\ $","$\\mu_1 \\quad$", "$\\sigma \\quad$", "$\\xi \\quad$")
pander(tab1,split.table = Inf)
```

At first sight, we can appreciate that results seem very similar to the frequentists ones. Let's now compare them, by taking the smaple mean value of the posterior for the Bayesian estimate.

We can also represent the **posterior densities** to have visual insight of the posterior's probability mass. (can smooth it)

```{r, fig.height=5.5, fig.width=8.5}
## Summary And Parameter Table
param.chain$sigma <- exp(param.chain$logsig)
tab_quantiles <- as.data.frame(summary(mcmc(param.chain))$quantiles)

## HPD intervals
library(HDInterval)
hpd_mu0 <- hdi(param.chain$mu0)
hpd_mu1 <- hdi(param.chain$mu1)
hpd_logsigma <- hdi(param.chain$logsig)
hpd_xi <- hdi(param.chain$xi)
hpd_sigma <- hdi(param.chain$sigma)
hpd95 <- data.frame(mu0 = c(hpd_mu0), mu1 = c(hpd_mu1),
                    logsig = c(hpd_logsigma),
                    xi = c(hpd_xi), sig = c(hpd_sigma))


library(gridExtra)

## Densities of the parameters with their quantile-based and  HPD 0.95 intervals
color_scheme_set("brightblue")
col.intervals <- c("Quantile" = "red", "HPD" = "green")

'legend.things'  <-
  list(scale_color_manual(name = "Intervals", values = col.intervals),
    theme_piss(legend.position = c(0.92, 0.5)),
    theme(legend.background = element_rect(colour = "transparent",size = 0.5))
   )

g1 <- mcmc_dens(param.chain, pars = c("mu0")) +
  geom_vline(aes(xintercept = tab_quantiles['mu0', "2.5%"],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = tab_quantiles['mu0', "97.5%"],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_mu0[[1]],
             col = "HPD"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_mu0[[2]],
             col = "HPD"), linetype = "dashed") +
  legend.things
g2 <- mcmc_dens(param.chain, pars = c("logsig")) +
  geom_vline(aes(xintercept = tab_quantiles['logsig', "2.5%"],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = tab_quantiles['logsig', "97.5%"],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_logsigma[[1]],
             col = "HPD"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_logsigma[[2]],
             col = "HPD"), linetype = "dashed") +
  legend.things
g3 <- mcmc_dens(param.chain, pars = c("xi"))+
  geom_vline(aes(xintercept = tab_quantiles['xi', '2.5%'],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = tab_quantiles['xi', "97.5%"],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_xi[[1]],
             col = "HPD"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_xi[[2]],
             col = "HPD"), linetype = "dashed")+
  legend.things
g4 <- mcmc_dens(param.chain, pars = c("mu1")) +
  geom_vline(aes(xintercept = tab_quantiles['mu1', '2.5%'],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = tab_quantiles['mu1', "97.5%"],
             col = "Quantile"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_mu1[[1]],
             col = "HPD"), linetype = "dashed") +
  geom_vline(aes(xintercept = hpd_mu1[[2]],
             col = "HPD"), linetype = "dashed") +
  legend.things

title <- "Posterior densities of the parameters and Bayesian intervals"
grid.arrange( g1, g4, g2, g3, nrow = 2,
              top = grid::textGrob(title,
                                   gp = grid::gpar(col = "#33666C",
                                                   fontsize = 20,
                                                   font = 4), vjust = 0.4))
```



**Comparisons with Frequentist's results (GEV) :**

```{r}
par_gibbs_trend <- apply(gibbs.trend$out.chain[,c("mu0", "mu1", "logsig", "xi")],
                         2, mean) # Average over the (3) generated chains
par_gibbs_trend["sigma"] <- exp(par_gibbs_trend["logsig"] ) 

frame <- data.frame(Bayesian = par_gibbs_trend[c("mu0", "mu1", "sigma", "xi")], 'Frequentist(mle)' = gev_nonstatio$mle)
row.names(frame) = c("$\\mu \\ $","$\\underline{\\mu_1} \\quad$", "$\\sigma \\quad$", "$\\xi \\quad$")
knitr::kable(frame, align = 'l')
```

Again, these look very similar. We must **notice** that is natural and this is rather comforting as we used **non-informative priors** so far. ( $\rightarrow$ *study behaviour if we introduce information through priors ?* 



**why is the value of** $\mu_1$ **different with this obtained with frequentist ?**


 **Note that for** $\mathbf{\mu_{trend}}$, time has been rescaled to $\mathbf^{scaled}$, to give : 

$$\mathbf{\mu}_{trend} = \mu + \mu_1 \cdot \mathbf{t}^{scaled} \qquad\text{where}\qquad t_{i}^{scaled} = \frac{t_i - mean(t) }{|\mathbf{t}|}, $$ 
or look inside `log_post1()`.  ${|\mathbf{t}|}$ denotes heee the length of ${\mathbf{t}}$.
This scaling had to be done for good behaviour of the generated chains. Verifications have been made, and this is perhaps why it did not work with `evdbayes` (see below).


## Posterior Predictive Distribution 

The posterior predictive distribution (PPD) is the distribution for future predicted data based on the data you have already seen. So the posterior predictive distribution is basically used to predict new data values. 

$$\begin{aligned}
	\text{Pr}\{\tilde{X}<x\ | \ \mathbf{x}\}= &\int_{\Theta}\Pr\{\tilde{X}<x \ | \ \theta \} \cdot \pi(\theta|\boldsymbol{x})\cdot d\theta \\ 
	= & \ \mathbb{E}_{\theta|\boldsymbol{x}}\big[\Pr(\tilde{X}<x \ | \ \theta)\big].
	\end{aligned}$$

To obtain this, we will estimate it by generating samples from the posterior ditribution to form the PPD. 

```{r}
# See the function to generate predictive posterior samples
repl <- PissoortThesis::pred_post_samples()

post.pred <- apply(repl, 2, function(x) quantile(x, probs = c(0.05,0.5,0.95)))


df.postpred <- data.frame(data = max_years$data, q05 = post.pred["5%",],
                          q50 = post.pred["50%",], q95 = post.pred["95%",],
                          year = seq(1901:2016))
ggplot(df.postpred) + geom_point(aes(x = year, y = data)) +
  geom_line(aes(x = year, y = q05)) + geom_line(aes(x = year, y = q50)) +
  geom_line(aes(x = year, y = q95)) +
  ggtitle("Original data with PPD quantiles 5, 50 and 95%") + theme_piss()

```


To compute this PPD from , we expressly took a range of $116$ years in the future, as we know that it is not recommended to make very long-term extrapolations. We first depict the results in Figure  where we represent the PPD by its $95\%$ credible intervals, together with the observed values from $1901$ to $2016$ and the values from $2016$ to $2131$ that have been simulated from this PPD .

```{r, fig.width=9}
n_future <- 116
repl2 <- PissoortThesis::pred_post_samples(n_future = n_future, seed = 12)

post.pred2 <- apply(repl2, 2, function(x) quantile(x, probs = c(0.025,0.5,0.975)))
hpd_pred <- as.data.frame(t(hdi(repl2)))

df.postpred2 <- data.frame(org.data = c(max_years$data,
                                        repl2[sample(10, 1:nrow(repl2)),
                                              117:(116+n_future)] ),
                           q025 = post.pred2["2.5%",], q50 = post.pred2["50%",],
                           q975 = post.pred2["97.5%",], year = 1901:(2016+n_future),
                           'data' = c(rep('original', 116), rep('new', n_future)),
                           hpd.low = hpd_pred$lower, hpd.up = hpd_pred$upper)

col.interval <- c("2.5%-97.5%" = "red", "Median" = "blue2", "HPD 95%" = "green2",
                  "orange", "magenta")
col.data <- c("original" = "cyan", "simulated" = "red", "orange", "magenta")

g.ppd <- ggplot(df.postpred2) +
  geom_line(aes(x = year, y = q025, col = "2.5%-97.5%"), linetype = "dashed") +
  geom_line(aes(x = year, y = q50, col = "Median")) +
  geom_line(aes(x = year, y = q975, col =  "2.5%-97.5%"), linetype = "dashed") +
  geom_line(aes(x = year, y = hpd.low, col = "HPD 95%"), linetype = "dashed") +
  geom_line(aes(x = year, y = hpd.up , col =  "HPD 95%"), linetype = "dashed") +
  geom_vline(xintercept = 2016, linetype = "dashed", size = 0.4, col  = 1) +
  scale_x_continuous(breaks = c(1900, 1950, 2000, 2016, 2050, 2100, 2131),
                     labels = c(1900, 1950, 2000, 2016, 2050, 2100, 2131) ) +
  scale_colour_manual(name = " PP intervals", values = col.interval) +
  geom_point(data = df.postpred2[1:116,],
             aes(x = year, y = org.data), col = "black" ) +
  geom_point(data = df.postpred2[117:nrow(df.postpred2),],
             aes(x = year, y = org.data), col = "orange" ) +
  scale_fill_discrete(name = "Data" ) + #, values = col.data) +
  labs(y = expression( Max~(T~degree*C)), x = "Year",
       title = "Posterior Predictive quantiles with observation + 116 years simulations") +
  theme(legend.position =  c(0.91, 0.12),
        plot.title = element_text(size = 28, colour = "#33666C",
                                  face="bold", hjust = 0.5),
        axis.title = element_text(size = 19, colour = "#33666C", face="bold"),
        legend.title = element_text(size = 19, colour = "#33666C",face="bold") )

## LEngth of the intervals
length.quantil <- df.postpred2$q975 - df.postpred2$q025
length.hpd <- df.postpred2$hpd.up - df.postpred2$hpd.low
df.length.ci <- data.frame(quantiles = length.quantil,
                           hpd = length.hpd,
                           Year = df.postpred2$year)

g.length <- ggplot(df.length.ci) +
  geom_line(aes(x = Year , y = quantiles), col = "red") +
  geom_line(aes(x = Year , y = hpd), col = "green2") +
  labs(title = "Intervals' lengths", y = "Length") +
  scale_x_continuous(breaks = c(1900, 1950, 2000, 2050, 2100, 2131),
                     labels = c(1900, 1950, 2000, 2050, 2100, 2131) ) +
  geom_vline(xintercept = 2016, linetype = "dashed", size = 0.4, col  = 1) +
  theme(plot.title = element_text(size = 17, colour = "#33666C",
                                  face="bold", hjust = 0.5),
        axis.title = element_text(size = 10, colour = "#33666C", face="bold"))

vp <- grid::viewport(width = 0.23,
                    height = 0.28,
                    x = 0.65,
                    y = 0.23)
g.ppd
print(g.length, vp = vp)
```

 We see in Figure the linear trend from the linear model on the location parameter of the posterior distribution.  Indeed, we see from the above equation the contribution of the posterior $\pi(\theta|\boldsymbol{x})$ to the PPD. 
 This Figure  also clearly highlights that the PP intervals are not $95\%$ credible intervals for the observed values but rather intervals for the posterior predicted values. Indeed, coverage analysis shows that the PP intervals cover $95\%$ of the simulated values from the PPD as the number of simulations becomes very high, but only $\approx 50\%$ for the observed values. The HPD and the quantile-based credible intervals are very similar, for all the observations or simulations. 
  However, we can see that these intervals are taking the uncertainty of predicting the future into account as they exponentially increase beyond the range of data. 
 In fact, the evolution of the PP quantiles is linear when in the range of data, and so is the PP median even beyond the range of the data. But, in extrapolation, the PP upper quantiles will have an increasing slope over time while the PP lower quantiles will have a decreasing slope.
 
 To better understand the PPD with a global view and in order to obtain a more convenient visual quantification of the predictive uncertainty over time, we present the following Figure : 
 
```{r, fig.height=8}
library(ggjoy)
library(viridis)
## Provide better visualizatons with geom_joy(). ( include it in the package !!!)
## Definition of parameters is straightfoward : it defines time at which we predict
# An by = is the number of densities we want to draw !
'posterior_pred_ggplot' <- function(from = 1, until = nrow(max_years$df),
                                    n_future = 0, by = 10, x_coord = c(27,35)) {

  repl2 <- PissoortThesis::pred_post_samples(from = from, until = until,
                                             n_future = n_future)

  repl2_df <- data.frame(repl2)
  colnames(repl2_df) <- seq(from + 1900, length = ncol(repl2))

  ## Compute some quantiles to later draw on the plot
  quantiles_repl2 <- apply(repl2_df, 2,
                           function(x) quantile(x , probs = c(.025, 0.05,
                                                              0.5, 0.95, 0.975)) )
  quantiles_repl2 <- as.data.frame(t(quantiles_repl2))
  quantiles_repl2$year <- colnames(repl2_df)

  repl2_df_gg <- repl2_df[, seq(1, (until + n_future) - from, by = by)] %>%
    gather(year, value)

  col.quantiles <- c("2.5%-97.5%" = "red", "Median" = "black", "HPD 95%" = "green2")

  last_year <- as.character((until + n_future) - from + 1900)
  titl <- paste("Posterior Predictive densities evolution in [ 1901 -", last_year,"] with linear model on location")
  subtitl <- paste("with some quantiles and intervals. The last density is in year ", last_year, ". After 2016 is extrapolation.")

  #browser()
  ## Compute the HPD intervals
  hpd_pred <- as.data.frame(t(hdi(repl2)))
  hpd_pred$year <- colnames(repl2_df)

  g <- ggplot(repl2_df_gg, aes(x = value, y = as.numeric(year) )) +  # %>%rev() inside aes()
    geom_joy(aes(fill = year)) +
    geom_point(aes(x = `2.5%`, y = as.numeric(year), col = "2.5%-97.5%"),
               data = quantiles_repl2, size = 0.9) +
    geom_point(aes(x = `50%`, y = as.numeric(year), col = "Median"),
               data = quantiles_repl2, size = 0.9) +
    geom_point(aes(x = `97.5%`, y = as.numeric(year), col = "2.5%-97.5%"),
               data = quantiles_repl2, size = 0.9) +
    geom_point(aes(x = lower, y = as.numeric(year) , col = "HPD 95%"),
               data = hpd_pred, size = 0.9) +
    geom_point(aes(x = upper, y = as.numeric(year) , col = "HPD 95%"),
               data = hpd_pred, size = 0.9) +
    geom_hline(yintercept = 2016, linetype = "dashed", size = 0.3, col  = 1) +
    scale_fill_viridis(discrete = T, option = "D", direction = -1, begin = .1, end = .9) +
    scale_y_continuous(breaks = c(  seq(1901, 2016, by = by),
      seq(2016, colnames(repl2_df)[ncol(repl2_df)], by = by) ) )  +
    coord_cartesian(xlim = x_coord) +
    theme_piss(theme = theme_minimal()) +
    labs(x = expression( Max~(T~degree*C)), y = "Year",
         title = titl, subtitle = subtitl) +
    scale_colour_manual(name = "Intervals", values = col.quantiles) +
    guides(colour = guide_legend(override.aes = list(size=4))) +
    theme(legend.position = c(.952, .37),
          plot.subtitle = element_text(hjust = 0.5,face = "italic"),
          plot.caption = element_text(hjust = 0.1,face = "italic"))
  g
}


## All
posterior_pred_ggplot(from = 1, x_coord = c(27, 38),
                      n_future = nrow(max_years$df), by = 12)
```
 
  From this Figure,  we also notice the linear trend and we visualize more clearly the evolution of the quantiles over time with their gap increasing after $2016$, i.e. from their deviation to the median. This comes from the predictive density distributions that become more and more flat after $2016$, indicating an increasing variance and hence the inclusion of the prediction uncertainty through the PPD. 
 
 Interesting information can come from these PP plots when considering other models, and the shape of the predictive densities is sometimes interesting. But, the parametric models considered are rather limited, and other models need to be developed. For example, step-change models, or more flexible models by following the idea of . It would then be interesting to consider \emph{Bayesian Neural Networks}.

**To be continued** : $\underline{\boldsymbol{\text{Further 'diagnostics' on the posterior predictive accuracy}}} :$

# Shiny Applications 

```{r}
knitr::include_app("https://proto4426.shinyapps.io/Bayesian_GibbsCpp/", height = "900px")
```

The code is available [here](https://github.com/proto4426/PissoortThesis/tree/master/inst/shiny-examples/Bayesian)

# References


